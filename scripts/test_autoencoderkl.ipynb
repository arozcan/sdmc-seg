{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.5.0+cu124\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: /home/<username>/miniconda3/envs/dpm/lib/python3.9/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.1\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.13.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.18.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.20.0+cu124\n",
      "tqdm version: 4.66.5\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.45.2\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import monai\n",
    "from monai.data import ArrayDataset, create_test_image_2d, decollate_batch,list_data_collate, DataLoader\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric, MeanIoU\n",
    "from monai.networks.nets import UNet, SegResNet, flexible_unet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    RandSpatialCropd,\n",
    "    RandRotate90d,\n",
    "    ScaleIntensityd,\n",
    "    Resized,\n",
    "    ToTensor,\n",
    "    Transposed,\n",
    "    Lambda,\n",
    "    Lambdad,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "from utils.image_processing_utils import overlay_prediction, overlay_images\n",
    "from utils.data_utils import prepare_dataloaders_stroke_2021\n",
    "\n",
    "monai.config.print_config()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "\n",
    "from ldm.models.autoencoder import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "Restored from logs/2024-10-31T00-42-15_autoencoder_kl_f8/checkpoints/epoch=000040.ckpt\n"
     ]
    }
   ],
   "source": [
    "lossconfig = {\n",
    "    \"target\": \"ldm.modules.losses.contperceptual.LPIPSWithDiscriminator\",\n",
    "    \"params\": {\n",
    "        \"disc_start\": 50001,\n",
    "        \"kl_weight\": 1.0e-06,\n",
    "        \"disc_weight\": 0.5,\n",
    "    }\n",
    "}\n",
    "\n",
    "model = AutoencoderKL(\n",
    "    ddconfig={\n",
    "        'double_z': True,\n",
    "        'z_channels': 4,\n",
    "        'resolution': 256,\n",
    "        'in_channels': 3,\n",
    "        'out_ch': 3,\n",
    "        'ch': 128,\n",
    "        'ch_mult': [1, 2, 4, 4],\n",
    "        'num_res_blocks': 2,\n",
    "        'attn_resolutions': [],\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    lossconfig=lossconfig,\n",
    "    embed_dim=4,\n",
    "    #ckpt_path=\"models/first_stage_models/kl-f8/model.ckpt\",\n",
    "    ckpt_path=\"logs/2024-12-16T16-18-19_autoencoder_kl_f8_stroke_image/checkpoints/epoch=57-step=253299.ckpt\",\n",
    "    image_key=\"image_dicom\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriseti klasörü\n",
    "data_dir = \"/home/arms/Workspace/Dataset/Stroke2021\"\n",
    "roi_size = (256, 256)\n",
    "spatial_size=[256,256]\n",
    "num_folds=5\n",
    "num_workers=0\n",
    "batch_size=1\n",
    "random_state=0\n",
    "\n",
    "# define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "train_loader, val_loader, class_weights = prepare_dataloaders_stroke_2021(data_dir, roi_size=roi_size, spatial_size=spatial_size, expand_img_channel=False, num_folds=1, val_ratio=0.2, num_workers=num_workers ,batch_size=batch_size, random_state=random_state, cache_rate=0.0, shuffle_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(image, reconstructed, idx=0):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "    # Show original image channels\n",
    "    axs[0, 0].imshow(image[idx, 0].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[0, 0].set_title(\"Input Image - Channel 1\")\n",
    "    axs[0, 0].axis('off')\n",
    "\n",
    "    axs[0, 1].imshow(image[idx, 1].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[0, 1].set_title(\"Input Image - Channel 2\")\n",
    "    axs[0, 1].axis('off')\n",
    "\n",
    "    axs[0, 2].imshow(image[idx, 2].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[0, 2].set_title(\"Input Image - Channel 3\")\n",
    "    axs[0, 2].axis('off')\n",
    "\n",
    "    # Show reconstructed image channels\n",
    "    axs[1, 0].imshow(reconstructed[idx, 0].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[1, 0].set_title(\"Reconstructed - Channel 1\")\n",
    "    axs[1, 0].axis('off')\n",
    "\n",
    "    axs[1, 1].imshow(reconstructed[idx, 1].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[1, 1].set_title(\"Reconstructed - Channel 2\")\n",
    "    axs[1, 1].axis('off')\n",
    "\n",
    "    axs[1, 2].imshow(reconstructed[idx, 2].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[1, 2].set_title(\"Reconstructed - Channel 3\")\n",
    "    axs[1, 2].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dice_metric_all = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "dice_metric_wob = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "iou_metric_all = MeanIoU(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "iou_metric_wob = MeanIoU(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "iou_metric_classwise = MeanIoU(include_background=True, reduction=\"none\", get_not_nans=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_masks(masks, num_classes, device):\n",
    "    \"\"\"\n",
    "    Given a batch of masks, this function converts them to one-hot encoded format.\n",
    "    \n",
    "    Args:\n",
    "    - masks (torch.Tensor): Input masks of shape [batch_size, 1, height, width].\n",
    "    - num_classes (int): The number of segmentation classes.\n",
    "    - device (torch.device): The device on which the operations should be performed (e.g., GPU or CPU).\n",
    "    \n",
    "    Returns:\n",
    "    - masks_onehot (torch.Tensor): One-hot encoded masks with shape [batch_size, num_classes, height, width].\n",
    "    \"\"\"\n",
    "    # 1. Kanal boyutunu sıkıştır (1. boyutu kaldır)\n",
    "    masks = masks.squeeze(1)  # [batch_size, height, width]\n",
    "\n",
    "    # 2. One-hot kodlama\n",
    "    masks_onehot = F.one_hot(masks.long(), num_classes=num_classes)  # [batch_size, height, width, num_classes]\n",
    "\n",
    "    # 3. Boyutları ayarla (kanalı öne taşı)\n",
    "    masks_onehot = masks_onehot.permute(0, 3, 1, 2).float().to(device)  # [batch_size, num_classes, height, width]\n",
    "    #masks_onehot = (masks_onehot*2.0)-1.0\n",
    "\n",
    "    return masks_onehot\n",
    "\n",
    "def apply_threshold_to_channels(batch_images, thresholds=[0.5, 0.5, 0.5]):\n",
    "    \"\"\"\n",
    "    Çoklu batch ve 3 kanallı bir görüntüde her bir kanala verilen threshold'u uygular ve binary hale getirir.\n",
    "    \n",
    "    Args:\n",
    "        batch_images (torch.Tensor): [B, C, H, W] şeklinde, B batch boyutu, C kanal sayısı, H ve W ise görüntü boyutları.\n",
    "        thresholds (list): Her kanal için uygulanacak threshold değerleri (örneğin, [0.5, 0.3, 0.7]).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Her kanala threshold uygulandıktan sonra binary hale getirilmiş tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = batch_images.device  # batch_images hangi cihazda ise onu al\n",
    "    binary_images = (batch_images >= torch.tensor(thresholds).view(1, -1, 1, 1).to(device)).float()\n",
    "    \n",
    "    return binary_images\n",
    "\n",
    "def one_hot_encode_nearest(reconstructed_image, num_classes=3):\n",
    "    \"\"\"\n",
    "    Converts a single-channel image to a one-hot encoded tensor based on the nearest value.\n",
    "    \n",
    "    Args:\n",
    "        reconstructed_image (torch.Tensor): [B, 1, H, W] shaped tensor (B: batch, H: height, W: width).\n",
    "        num_classes (int): The number of classes for one-hot encoding (default: 3 for values 0, 1, 2).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: One-hot encoded tensor, shape [B, num_classes, H, W].\n",
    "    \"\"\"\n",
    "    closest_values = torch.argmin(torch.abs(reconstructed_image - torch.arange(num_classes, device=reconstructed_image.device).view(1, num_classes, 1, 1)), dim=1)\n",
    "    one_hot_encoded = torch.nn.functional.one_hot(closest_values, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
    "    return one_hot_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1331/1331 [04:37<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean dice (all): 0.9988, Mean dice (wo backgorund) 0.9928\n",
      "Mean iou (all): 0.9976, Mean iou (wo backgorund) 0.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=3)\n",
    "post_mask = AsDiscrete(to_onehot=3)\n",
    "dice_metric_all.reset()\n",
    "dice_metric_wob.reset()\n",
    "iou_metric_all.reset()\n",
    "iou_metric_wob.reset()\n",
    "for test_data in tqdm(val_loader, total=len(val_loader), desc=f\"Test\"):\n",
    "        val_images, val_masks = test_data[\"image_dicom\"].to(device), test_data[\"mask\"].to(device)\n",
    "        val_masks_hot_encoded=one_hot_encode_masks(val_masks,3,device)\n",
    "        with torch.no_grad():\n",
    "            posterior = model.encode(val_masks_hot_encoded)\n",
    "            latent = posterior.sample()\n",
    "            reconstructed_image = model.decode(latent)\n",
    "            reconstructed_image_binary = apply_threshold_to_channels(reconstructed_image)\n",
    "            dice_metric_all(y_pred=reconstructed_image_binary, y=val_masks_hot_encoded)\n",
    "            dice_metric_wob(y_pred=reconstructed_image_binary, y=val_masks_hot_encoded)\n",
    "            iou_metric_all(y_pred=reconstructed_image_binary, y=val_masks_hot_encoded)\n",
    "            iou_metric_wob(y_pred=reconstructed_image_binary, y=val_masks_hot_encoded)\n",
    "            #show_images(val_masks_hot_encoded, reconstructed_image_binary)\n",
    "dice_metric_all_mean = dice_metric_all.aggregate().item()\n",
    "dice_metric_wob_mean = dice_metric_wob.aggregate().item()\n",
    "iou_metric_all_mean = iou_metric_all.aggregate().item()\n",
    "iou_metric_wob_mean = iou_metric_wob.aggregate().item()\n",
    "print(f\"Mean dice (all): {dice_metric_all_mean:.4f}, Mean dice (wo backgorund) {dice_metric_wob_mean:.4f}\")\n",
    "print(f\"Mean iou (all): {iou_metric_all_mean:.4f}, Mean iou (wo backgorund) {iou_metric_wob_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval() \n",
    "# post_pred = AsDiscrete(argmax=True, to_onehot=3)\n",
    "# post_mask = AsDiscrete(to_onehot=3)\n",
    "# dice_metric.reset()\n",
    "# for test_data in tqdm(val_loader[0], total=len(val_loader[0]), desc=f\"Test\"):\n",
    "#         val_images, val_masks = test_data[\"image_dicom\"].to(device), test_data[\"mask\"].to(device)\n",
    "#         val_masks_repated=val_masks.repeat(1, 3, 1, 1).to(device)\n",
    "#         val_masks_hot_encoded=one_hot_encode_masks(val_masks,3,device)\n",
    "#         with torch.no_grad():\n",
    "#             posterior = model.encode(val_masks_repated)\n",
    "#             latent = posterior.sample()\n",
    "#             reconstructed_image = model.decode(latent)\n",
    "#             reconstructed_image_binary = one_hot_encode_nearest(reconstructed_image)\n",
    "#             dice_metric(y_pred=reconstructed_image_binary, y=val_masks_hot_encoded)\n",
    "#             #show_images(val_masks_hot_encoded, reconstructed_image_binary)\n",
    "# dice_metric_mean = dice_metric.aggregate().item()\n",
    "# print(f\"Mean dice: {dice_metric_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=3)\n",
    "post_mask = AsDiscrete(to_onehot=3)\n",
    "dice_metric.reset()\n",
    "for test_data in tqdm(val_loader[0], total=len(val_loader[0]), desc=f\"Test\"):\n",
    "        val_images, val_masks = test_data[\"image_dicom\"].to(device), test_data[\"mask\"].to(device)\n",
    "        val_masks_repated=val_masks.repeat(1, 3, 1, 1).to(device)\n",
    "        val_masks_repated = torch.clamp(val_masks_repated, max=1)\n",
    "        #val_masks_hot_encoded=one_hot_encode_masks(val_masks,3,device)\n",
    "        with torch.no_grad():\n",
    "            posterior = model.encode(val_masks_repated)\n",
    "            latent = posterior.sample()\n",
    "            reconstructed_image = model.decode(latent)\n",
    "            reconstructed_image_binary = apply_threshold_to_channels(reconstructed_image)\n",
    "            dice_metric(y_pred=reconstructed_image_binary[:,0,:,:], y=val_masks_repated[:,0,:,:])\n",
    "            #show_images(val_masks_repated, reconstructed_image_binary)\n",
    "dice_metric_mean = dice_metric.aggregate().item()\n",
    "print(f\"Mean dice: {dice_metric_mean:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
